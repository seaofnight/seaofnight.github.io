<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ceph on Dev Blog</title>
    <link>https://seaofnight.github.io/categories/ceph/</link>
    <description>Recent content in ceph on Dev Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>.</copyright>
    <lastBuildDate>Sun, 07 Jun 2020 10:58:08 -0400</lastBuildDate>
    
	<atom:link href="https://seaofnight.github.io/categories/ceph/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ceph create file system and mount </title>
      <link>https://seaofnight.github.io/posts/ceph-create-filesystem/</link>
      <pubDate>Sun, 07 Jun 2020 10:58:08 -0400</pubDate>
      
      <guid>https://seaofnight.github.io/posts/ceph-create-filesystem/</guid>
      <description>개요  ceph version 은 15.2 입니다.  create replicated pool ceph orch apply mds cephfs --placement=&amp;#34;ceph0,ceph1,ceph2&amp;#34; FS_NAME=cephfs_repl_data FS_META=cephfs_repl_meta PG_CNT=8 ceph osd pool create ${FS_NAME} ${PG_CNT} ceph osd pool create ${FS_META} ${PG_CNT} ceph fs new cephfs ${FS_META} ${FS_NAME} [root@ceph-test mnt]# ceph fs status cephfs - 0 clients ====== RANK STATE MDS ACTIVITY DNS INOS 0 active cephfs.ceph1.witvio Reqs: 0 /s 10 13 POOL TYPE USED AVAIL cephfs_repl_meta metadata 1536k 410G cephfs_repl_data data 0 410G STANDBY MDS cephfs.</description>
    </item>
    
    <item>
      <title>ceph create rbd and mount </title>
      <link>https://seaofnight.github.io/posts/ceph-create-rbd/</link>
      <pubDate>Sun, 07 Jun 2020 10:58:08 -0400</pubDate>
      
      <guid>https://seaofnight.github.io/posts/ceph-create-rbd/</guid>
      <description>개요  ceph version 은 15.2 입니다.  ceph create replicated pool # pg calcualte exp : 100 = (3 * 100) / 3 ## 대략 7.x 가 나와서 default 옵션인 8로 진행 ## (OSDs * 100) ##Total PGs = ------------ ## pool size ceph osd pool create rbd_repl_bench 8 8 # rbd create \ # ${IMAGE_NAME:=img_repl_bench} \ # --size ${10G, 1024:default unit is MB} \ # --pool ${POOL_NAME:=rbd_repl_bench} rbd create img_repl_bench --size 102400 --pool rbd_repl_bench # rbd 의 기능을 비활성 화 하여야만이 image 를 host 에 map 할 수 있다.</description>
    </item>
    
    <item>
      <title>rook-ceph 설치 및 rbd 연동 </title>
      <link>https://seaofnight.github.io/posts/rook-ceph-install/</link>
      <pubDate>Sun, 07 Jun 2020 10:58:08 -0400</pubDate>
      
      <guid>https://seaofnight.github.io/posts/rook-ceph-install/</guid>
      <description>Install git clone https://github.com/rook/rook.git cd cluster/examples/kubernetes/ceph kubectl create -f common.yaml kubectl create -f operator.yaml # 아래는 예시입니다. ## ceph cluster 설정에 맞추어서 변경을 해주셔야 합니다. cat &amp;lt;&amp;lt;EOF | kubectl create -f - --- apiVersion: ceph.rook.io/v1 kind: CephCluster metadata: name: rook-ceph namespace: rook-ceph spec: cephVersion: image: ceph/ceph:v14.2.4-20190917 allowUnsupported: true dataDirHostPath: /var/lib/rook skipUpgradeChecks: false mon: count: 1 allowMultiplePerNode: true dashboard: enabled: true monitoring: enabled: false # requires Prometheus to be pre-installed rulesNamespace: rook-ceph network: hostNetwork: false rbdMirroring: workers: 0 mgr: modules: # the pg_autoscaler is only available on nautilus or newer.</description>
    </item>
    
    <item>
      <title>ceph ansible simple installation</title>
      <link>https://seaofnight.github.io/posts/ceph-install-with-ansible/</link>
      <pubDate>Mon, 01 Jun 2020 10:58:08 -0400</pubDate>
      
      <guid>https://seaofnight.github.io/posts/ceph-install-with-ansible/</guid>
      <description>개요  회사에서 Ceph 를 계속 스터디를 하고 있었으나. K8s Cluster 에 올려놓고 사용만 하고있었지 Baremetal 에 올려 사용하지는 않고있었다. 자세하게 사용하기도 해야할 뿐더러 여러가지 옵션을 테스트 및 운용하기 위한 클러스터 구축을 정리하기 위하여 본 포스팅을 정리한다. Ceph 설치방법은 Ceph-ansible 이며 stable-3.2 버전을 이용하여 설치 하였다. 설치 노드는  mon 1, osd 3, mgr 1 이며 mon1 과 mgr1, osd1 이 같은 호스트에 배포가 되었다. ceph0 10.0.3.2 8cpu 16gb ram, /dev/sdb 300gb ceph1 10.</description>
    </item>
    
    <item>
      <title>ceph commands</title>
      <link>https://seaofnight.github.io/posts/ceph-commands/</link>
      <pubDate>Mon, 01 Jun 2020 10:58:08 -0400</pubDate>
      
      <guid>https://seaofnight.github.io/posts/ceph-commands/</guid>
      <description>ceph status [root@ceph0 ~]# ceph -s cluster: id: 0ad3892c-c903-44d3-8872-b7536d10bcdb health: HEALTH_OK services: mon: 1 daemons, quorum ceph0 mgr: ceph0(active) osd: 3 osds: 3 up, 3 in data: pools: 5 pools, 258 pgs objects: 0 objects, 0B usage: 3.01GiB used, 897GiB / 900GiB avail pgs: 258 active+clean # watch cluster health [root@ceph0 ~]# ceph -w cluster: id: 0ad3892c-c903-44d3-8872-b7536d10bcdb health: HEALTH_OK services: mon: 1 daemons, quorum ceph0 mgr: ceph0(active) osd: 3 osds: 3 up, 3 in data: pools: 5 pools, 258 pgs objects: 0 objects, 0B usage: 3.</description>
    </item>
    
  </channel>
</rss>